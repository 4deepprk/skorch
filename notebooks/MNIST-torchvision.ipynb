{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST with torchvision and skorch\n",
    "\n",
    "This notebooks shows how to define and train a simple Neural-Network with PyTorch and use it via skorch with the help of torchvision.\n",
    "\n",
    "<table align=\"left\"><td>\n",
    "<a target=\"_blank\" href=\"https://colab.research.google.com/github/dnouri/skorch/blob/master/notebooks/MNIST.ipynb\">\n",
    "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>  \n",
    "</td><td>\n",
    "<a target=\"_blank\" href=\"https://github.com/dnouri/skorch/blob/master/notebooks/MNIST.ipynb\"><img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a></td></table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**: If you are running this in [a colab notebook](https://colab.research.google.com/github/dnouri/skorch/blob/master/notebooks/MNIST.ipynb), we recommend you enable a free GPU by going:\n",
    "\n",
    "> **Runtime**   →   **Change runtime type**   →   **Hardware Accelerator: GPU**\n",
    "\n",
    "If you are running in colab, you should install the dependencies and download the dataset by running the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "! [ ! -z \"$COLAB_GPU\" ] && pip install torch scikit-learn==0.20.* skorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import islice\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision.datasets import MNIST\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data\n",
    "\n",
    "Use torchvision's data repository to provide MNIST data in form of a torch `Dataset`. Originally, the `MNIST` dataset provides 28x28 `PIL` images. To use them with PyTorch, we convert those to tensors by adding the `ToTensor` transform."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train = MNIST('datasets', train=True, download=True, transform=torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test = MNIST('datasets', train=False, download=True, transform=torchvision.transforms.Compose([\n",
    "    torchvision.transforms.ToTensor(),\n",
    "]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking a look at the data\n",
    "\n",
    "Each entry in the `mnist_train` and `mnist_test` Dataset instances consists of a 28 x 28 images and the corresponding label (numbers between 0 and 9). The image data is already normalized to the range [0; 1]. Let's take a look at the first 5 images of the training set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_example, y_example = zip(*islice(iter(mnist_train), 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.), tensor(1.))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_example[0].min(), X_example[0].max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print a selection of training images and their labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_example(X, y, n=5):\n",
    "    \"\"\"Plot the images in X and their labels in rows of `n` elements.\"\"\"\n",
    "    fig = plt.figure()\n",
    "    rows = len(X) // n + 1\n",
    "    for i, (img, y) in enumerate(zip(X, y)):\n",
    "        ax = fig.add_subplot(rows, n, i + 1)\n",
    "        ax.imshow(img.reshape(28, 28))\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_title(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f145f4a67f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_example(torch.stack(X_example), y_example, n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing a validation split\n",
    "\n",
    "skorch can split the data for us automatically but since we are using `Dataset`s for their lazy-loading property there is no way skorch can do a stratified split automatically without exploring the data completely first (which it doesn't). \n",
    "\n",
    "If we want skorch to do a validation split for us we need to retrieve the `y` values from the dataset and pass these values to `net.fit` later on:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = np.array([y for x, y in iter(mnist_train)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Neural Network with PyTorch\n",
    "\n",
    "Simple, fully connected neural network with one hidden layer. Input layer has 784 dimensions (28x28), hidden layer has 98 (= 784 / 8) and output layer 10 neurons, representing digits 0 - 9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "mnist_flat_dim = 28 * 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple neural network classifier with linear layers and a final softmax in PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ClassifierModule(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            input_dim=mnist_flat_dim,\n",
    "            hidden_dim=98,\n",
    "            output_dim=10,\n",
    "            dropout=0.5,\n",
    "    ):\n",
    "        super(ClassifierModule, self).__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.hidden = nn.Linear(input_dim, hidden_dim)\n",
    "        self.output = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, X, **kwargs):\n",
    "        X = X.reshape(-1, self.hidden.in_features)\n",
    "        X = F.relu(self.hidden(X))\n",
    "        X = self.dropout(X)\n",
    "        X = F.softmax(self.output(X), dim=-1)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "skorch allows to use PyTorch with an sklearn API. We will train the classifier using the classic sklearn `.fit()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch import NeuralNetClassifier\n",
    "from skorch.dataset import CVSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "net = NeuralNetClassifier(\n",
    "    ClassifierModule,\n",
    "    max_epochs=10,\n",
    "    iterator_train__num_workers=4,\n",
    "    iterator_valid__num_workers=4,\n",
    "    lr=0.1,\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss     dur\n",
      "-------  ------------  -----------  ------------  ------\n",
      "      1        \u001b[36m0.7950\u001b[0m       \u001b[32m0.9000\u001b[0m        \u001b[35m0.3617\u001b[0m  5.4098\n",
      "      2        \u001b[36m0.4229\u001b[0m       \u001b[32m0.9209\u001b[0m        \u001b[35m0.2814\u001b[0m  5.3074\n",
      "      3        \u001b[36m0.3543\u001b[0m       \u001b[32m0.9299\u001b[0m        \u001b[35m0.2412\u001b[0m  5.4482\n",
      "      4        \u001b[36m0.3166\u001b[0m       \u001b[32m0.9380\u001b[0m        \u001b[35m0.2122\u001b[0m  4.8581\n",
      "      5        \u001b[36m0.2868\u001b[0m       \u001b[32m0.9447\u001b[0m        \u001b[35m0.1922\u001b[0m  4.9590\n",
      "      6        \u001b[36m0.2713\u001b[0m       \u001b[32m0.9485\u001b[0m        \u001b[35m0.1799\u001b[0m  5.1911\n",
      "      7        \u001b[36m0.2498\u001b[0m       \u001b[32m0.9516\u001b[0m        \u001b[35m0.1676\u001b[0m  4.9538\n",
      "      8        \u001b[36m0.2403\u001b[0m       \u001b[32m0.9537\u001b[0m        \u001b[35m0.1577\u001b[0m  5.2120\n",
      "      9        \u001b[36m0.2287\u001b[0m       \u001b[32m0.9559\u001b[0m        \u001b[35m0.1515\u001b[0m  4.6677\n",
      "     10        \u001b[36m0.2165\u001b[0m       \u001b[32m0.9574\u001b[0m        \u001b[35m0.1456\u001b[0m  5.2533\n"
     ]
    }
   ],
   "source": [
    "net.fit(mnist_train, y=y_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = net.predict(mnist_test)\n",
    "y_test = np.array([y for x, y in iter(mnist_test)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9576"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An accuracy of about 96% for a network with only one hidden layer is not too bad.\n",
    "\n",
    "Let's take a look at some predictions that went wrong.\n",
    "\n",
    "We compute the index of elements that are misclassified and plot a few of those to get an idea\n",
    "of what went wrong."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "error_mask = y_pred != y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have the mask we need a way to access the images from the `mnist_test` dataset. Luckily, skorch provides a helper class that lets us slice arbitrary `Dataset` objects, `SlicedDataset`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skorch.helper import SliceDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_test_sliceable = SliceDataset(mnist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred = torch.stack(list(mnist_test_sliceable[error_mask]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f146009d438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_example(X_pred[:5], y_pred[error_mask][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Network\n",
    "\n",
    "Next we want to turn it up a notch and use a convolutional neural network which is far better\n",
    "suited for images than simple densely connected layers.\n",
    "\n",
    "PyTorch expects a 4 dimensional tensor as input for its 2D convolution layer. The dimensions represent:\n",
    "\n",
    "* Batch size\n",
    "* Number of channels\n",
    "* Height\n",
    "* Width\n",
    "\n",
    "MNIST data only has one channel since there is no color information. As stated above, each MNIST vector represents a 28x28 pixel image. Hence, the resulting shape for the input tensor needs to be `(x, 1, 28, 28)` where `x` is the batch size and automatically provided by the data loader.\n",
    "\n",
    "Luckily, our data is already formated that way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 28, 28])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_example[0].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let us define the convolutional neural network module using PyTorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cnn(nn.Module):\n",
    "    def __init__(self, dropout=0.5):\n",
    "        super(Cnn, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3)\n",
    "        self.conv2_drop = nn.Dropout2d(p=dropout)\n",
    "        self.fc1 = nn.Linear(1600, 100) # 1600 = number channels * width * height\n",
    "        self.fc2 = nn.Linear(100, 10)\n",
    "        self.fc1_drop = nn.Dropout(p=dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = torch.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        \n",
    "        # flatten over channel, height and width = 1600\n",
    "        x = x.view(-1, x.size(1) * x.size(2) * x.size(3))\n",
    "        \n",
    "        x = torch.relu(self.fc1_drop(self.fc1(x)))\n",
    "        x = torch.softmax(self.fc2(x), dim=-1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As before we can wrap skorch's `NeuralNetClassifier` around our module and start training it like every other sklearn model using `.fit`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "cnn = NeuralNetClassifier(\n",
    "    Cnn,\n",
    "    max_epochs=10,\n",
    "    lr=0.0002,\n",
    "    optimizer=torch.optim.Adam,\n",
    "    device=device,\n",
    "    iterator_train__num_workers=4,\n",
    "    iterator_valid__num_workers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_loss    valid_acc    valid_loss      dur\n",
      "-------  ------------  -----------  ------------  -------\n",
      "      1        \u001b[36m0.9210\u001b[0m       \u001b[32m0.9295\u001b[0m        \u001b[35m0.2458\u001b[0m  22.8442\n",
      "      2        \u001b[36m0.3067\u001b[0m       \u001b[32m0.9572\u001b[0m        \u001b[35m0.1484\u001b[0m  23.7209\n",
      "      3        \u001b[36m0.2163\u001b[0m       \u001b[32m0.9649\u001b[0m        \u001b[35m0.1168\u001b[0m  25.0062\n",
      "      4        \u001b[36m0.1777\u001b[0m       \u001b[32m0.9694\u001b[0m        \u001b[35m0.1003\u001b[0m  25.2137\n",
      "      5        \u001b[36m0.1522\u001b[0m       \u001b[32m0.9738\u001b[0m        \u001b[35m0.0879\u001b[0m  23.9803\n",
      "      6        \u001b[36m0.1370\u001b[0m       \u001b[32m0.9764\u001b[0m        \u001b[35m0.0798\u001b[0m  24.8945\n",
      "      7        \u001b[36m0.1255\u001b[0m       \u001b[32m0.9791\u001b[0m        \u001b[35m0.0735\u001b[0m  24.3985\n",
      "      8        \u001b[36m0.1186\u001b[0m       \u001b[32m0.9796\u001b[0m        \u001b[35m0.0691\u001b[0m  24.9795\n",
      "      9        \u001b[36m0.1094\u001b[0m       \u001b[32m0.9804\u001b[0m        \u001b[35m0.0649\u001b[0m  25.1321\n",
      "     10        \u001b[36m0.1036\u001b[0m       \u001b[32m0.9821\u001b[0m        \u001b[35m0.0616\u001b[0m  25.6325\n"
     ]
    }
   ],
   "source": [
    "cnn.fit(mnist_train, y=y_train);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_cnn = cnn.predict(mnist_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9849"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test, y_pred_cnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An accuracy of >98% should suffice for this example!\n",
    "\n",
    "Let's see how we fare on the examples that went wrong before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7193396226415094"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(y_test[error_mask], y_pred_cnn[error_mask])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great success! The majority of the previously misclassified images are now correctly identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_pred_cnn = torch.stack(list(mnist_test_sliceable[error_mask]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f1417992048>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_example(X_pred_cnn[:5], y_pred_cnn[error_mask][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid searching parameter configurations\n",
    "\n",
    "Finally we want to show an example of how to use sklearn grid search when using torch `Dataset` instances.\n",
    "\n",
    "When doing k-fold validation grid search we have the same problem as before that sklearn is only able to do (stratified) splits when the data is sliceable. While skorch knows how to deal with PyTorch `Dataset` objects and only needs `y` to be known beforehand, sklearn doesn't know how to deal with `Dataset`s and needs a wrapper that makes them sliceable.\n",
    "\n",
    "Fortunately, we already know that skorch provides such a helper: `SliceDataset`.\n",
    "\n",
    "What is left to do is to define our parameter search space and run the grid search with a sliceable instance of `mnist_train`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'module__dropout': [0, 0.5, 0.8],\n",
    "    'max_epochs': [1],\n",
    "    'verbose': [False],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing module.\n",
      "Re-initializing optimizer.\n"
     ]
    }
   ],
   "source": [
    "cnn.initialize();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(cnn, param_grid=params, scoring='accuracy', verbose=1, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_train_sliceable = SliceDataset(mnist_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 3 candidates, totalling 9 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marian/anaconda3/envs/skorch/lib/python3.6/site-packages/sklearn/model_selection/_split.py:1943: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   9 out of   9 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv='warn', error_score='raise-deprecating',\n",
       "       estimator=<class 'skorch.classifier.NeuralNetClassifier'>[initialized](\n",
       "  module_=Cnn(\n",
       "    (conv1): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (conv2_drop): Dropout2d(p=0.5)\n",
       "    (fc1): Linear(in_features=1600, out_features=100, bias=True)\n",
       "    (fc2): Linear(in_features=100, out_features=10, bias=True)\n",
       "    (fc1_drop): Dropout(p=0.5)\n",
       "  ),\n",
       "),\n",
       "       fit_params=None, iid='warn', n_jobs=None,\n",
       "       param_grid={'module__dropout': [0, 0.5, 0.8], 'max_epochs': [1], 'verbose': [False]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=1)"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(mnist_train_sliceable, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After running the grid search we now know the best configuration in our search space:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_epochs': 1, 'module__dropout': 0, 'verbose': False}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
